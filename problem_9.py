# -*- coding: utf-8 -*-
"""Problem 9

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11bd-reF9hFATrwqCi36fc6kCZoGhh49D
"""

import numpy as np
# Assuming a function for truncated SVD (as in problem 8)
def truncated_svd(block, k):
    U, s, V = np.linalg.svd(block)
    return U[:, :k], np.diag(s[:k]), V[:k, :]

# Assuming a function for three-step motion estimation (simplified)
def three_step_motion_estimation(current_frame_block, reference_frame):
    # This is a very simplified placeholder. A real implementation is complex.
    # It would involve searching a neighborhood in the reference frame
    # for the best matching block based on some criteria (e.g., SAD, MSE).
    best_u = 0
    best_v = 0
    # In a real scenario, you'd return the actual (u, v) displacement
    return best_u, best_v

def video_filtering_motion_estimation(video_frames, ner, k_svd):
    filtered_video = []
    num_frames = len(video_frames)
    frame_index = 0

    while frame_index < num_frames:
        # 1. Process Reference Frame
        reference_frame = video_frames[frame_index]
        filtered_reference_frame = reference_frame.copy() # Placeholder

        # Apply truncated SVD to N x N blocks of the reference frame
        block_size = 8  # Example block size
        svd_coefficients = {} # Store SVD basis vectors

        for i in range(0, reference_frame.shape[0] - block_size + 1, block_size):
            for j in range(0, reference_frame.shape[1] - block_size + 1, block_size):
                block = reference_frame[i:i + block_size, j:j + block_size]
                U, S, V_t = truncated_svd(block, k_svd)
                svd_coefficients[(i, j)] = (U, S, V_t)

                # Reconstruct and store the filtered block for the reference frame
                filtered_block = U @ S @ V_t
                filtered_reference_frame[i:i + block_size, j:j + block_size] = filtered_block

        filtered_video.append(filtered_reference_frame)
        frame_index += 1

        # 2. Process Estimated Frames
        for _ in range(ner):
            if frame_index < num_frames:
                estimated_frame = video_frames[frame_index]
                filtered_estimated_frame = np.zeros_like(estimated_frame, dtype=float)

                for i in range(0, estimated_frame.shape[0] - block_size + 1, block_size):
                    for j in range(0, estimated_frame.shape[1] - block_size + 1, block_size):
                        current_block = estimated_frame[i:i + block_size, j:j + block_size]

                        # Motion Estimation
                        motion_u, motion_v = three_step_motion_estimation(current_block, reference_frame)

                        # Find the corresponding block in the reference frame
                        ref_block_row = i + motion_u
                        ref_block_col = j + motion_v

                        # Ensure the reference block coordinates are within bounds
                        if (ref_block_row >= 0 and ref_block_row <= reference_frame.shape[0] - block_size and
                                ref_block_col >= 0 and ref_block_col <= reference_frame.shape[1] - block_size):
                            ref_block_coords = (ref_block_row, ref_block_col)
                            if ref_block_coords in svd_coefficients:
                                U_ref, S_ref, _ = svd_coefficients[ref_block_coords]

                                # Project the current block onto the reference block's basis vectors
                                projection = current_block @ U_ref
                                approximated_block = projection @ U_ref.T

                                filtered_estimated_frame[i:i + block_size, j:j + block_size] = approximated_block
                            else:
                                # Handle cases where the motion vector points outside processed reference blocks
                                filtered_estimated_frame[i:i + block_size, j:j + block_size] = current_block # Or some other fallback

                        else:
                            # Handle out-of-bounds motion vectors
                            filtered_estimated_frame[i:i + block_size, j:j + block_size] = current_block # Or some other fallback

                filtered_video.append(filtered_estimated_frame)
                frame_index += 1
            else:
                break

    return np.array(filtered_video)

# Example usage (with dummy video frames)
if __name__ == "__main__":
    # Create some dummy video frames (replace with actual video loading)
    width, height = 128, 128
    num_frames = 10
    dummy_video = [np.random.rand(height, width) for _ in range(num_frames)]

    ner_value = 3
    k_svd_value = 10 # Number of singular values to keep

    filtered_video = video_filtering_motion_estimation(dummy_video, ner_value, k_svd_value)

    print(f"Original number of frames: {len(dummy_video)}")
    print(f"Filtered number of frames: {len(filtered_video)}")
    # You would then typically save or display the filtered video